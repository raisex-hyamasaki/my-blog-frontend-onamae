{"pageProps":{"article":{"id":268,"documentId":"oy8vkgnlt756zh041qbas6zt","title":"テキストのトークン数を確認する方法：LLMチューニングの基礎知識","content":"LLM（大規模言語モデル）のチューニングでは、トレーニングデータの品質だけでなく、トークン数が重要な役割を果たします。トークン数を把握する事で、データの適切な量を判断し、過学習を防ぐことが可能になります。この記事では、トークン数を確認するための主要なツールとその使い方を解説します。\n\n---\n\n# トークン数を確認する理由\n\nトークンとは、テキストを小さな単位に分割したものです。たとえば、単語や句読点、記号などが該当します。LLMのトレーニングでは、以下のような理由でトークン数を知ることが重要です。\n\n1. **適切なトレーニング量の判断**：トークン数が少ないとモデルが十分な学習を行えず、性能が低下する可能性があります。一方、データが多すぎると計算資源を無駄に消費します。\n2. **モデルの特性に合わせたチューニング**：モデルごとに最適なトークン数が異なるため、トークン化に対応したツールを使うことで効率的な学習を実現できます。\n\n---\n\n# トークン数を確認する手軽な方法\n\n以下では、トークン数を確認するための主要な方法を紹介します。それぞれのツールは特定の用途やモデルに適しているため、目的に応じて使い分けると良いでしょう。\n\n## 1. **tiktoken**\n\n### 特徴\n\n`tiktoken`はOpenAIが提供するトークナイザーで、GPTシリーズ向けに設計されています。モデルのエンコーディング設定を変更することで、異なるモデルに対応可能です。\n\n### 使用方法\n\n**インストール**\n\n```bash\npip install tiktoken\n```\n\n**ソースコード**\n\n```python\nimport tiktoken\n\ndef count_tokens(text):\n    tokenizer = tiktoken.get_encoding(\"o200k_base\") # gpt-4oモデル向け\n    tokens = tokenizer.encode(text)\n    return len(tokens)\n\n# テキストファイルを読み込みトークン数を確認\ninput_file_path = './text.txt'\nwith open(input_file_path, 'r', encoding='utf-8') as file:\n    raw_text = file.read()\n\ntoken_len = count_tokens(raw_text)\nprint(token_len)\n```\n\n**結果例**\n\n- `o200k_base`では1143トークン\n- `cl100k_base`に変更すると1420トークン\n\n---\n\n## 2. **Transformers**\n\n### 特徴\n\nHugging Faceの`transformers`ライブラリは、多くのモデルに対応したトークナイザーを提供しています。日本語モデルにも対応しているため、多言語テキストの解析に便利です。\n\n### 使用方法\n\n**インストール**\n\n```bash\npip install transformers\n```\n\n**ソースコード**\n\n```python\nfrom transformers import AutoTokenizer\n\ndef count_tokens(text):\n    tokenizer = AutoTokenizer.from_pretrained(\"llm-jp/llm-jp-3-3.7b-instruct\")\n    tokens = tokenizer.encode(text)\n    return len(tokens)\n\n# テキストファイルを読み込みトークン数を確認\ninput_file_path = './text.txt'\nwith open(input_file_path, 'r', encoding='utf-8') as file:\n    raw_text = file.read()\n\ntoken_len = count_tokens(raw_text)\nprint(token_len)\n\n```\n\n**結果例**\n\n- `llm-jp-3-3.7b-instruct`モデルでは991トークン\n- `elyza/ELYZA-japanese-Llama-2-7b-instruct`では1556トークン\n\n---\n\n## 3. **SpaCy**\n\n### 特徴\n\n`spaCy`は汎用的なNLPツールで、日本語を含む多言語のトークン化が可能です。LLM専用ではないため、簡易的な確認に適しています。\n\n### 使用方法\n\n**インストール**\n\n```bash\npip install spacy\npython -m spacy download ja_core_news_sm\n```\n\n**ソースコード**\n\n```python\nimport spacy\n\ndef count_tokens(text):\n    nlp = spacy.load(\"ja_core_news_sm\")\n    doc = nlp(text)\n    tokens = [token.text for token in doc]\n    return len(tokens)\n\n# テキストファイルを読み込みトークン数を確認\ninput_file_path = './text.txt'\nwith open(input_file_path, 'r', encoding='utf-8') as file:\n    raw_text = file.read()\n\ntoken_len = count_tokens(raw_text)\nprint(token_len)\n\n```\n\n**結果例**\n\n- トークン数は1118\n\n---\n\n## 4. **MeCab**\n\n### 特徴\n\n`MeCab`は日本語形態素解析ツールで、`fugashi`を通じてPythonから利用できます。日本語テキストの解析に特化しており、詳細なトークン化が可能です。\n\n### 使用方法\n\n**インストール**\n\n```bash\npip install 'fugashi[unidic]'\npython -m unidic download\n```\n\n**ソースコード**\n\n```python\nimport fugashi\n\ndef count_tokens(text):\n    tagger = fugashi.Tagger()\n    tokens = [word.surface for word in tagger(text)]\n    return len(tokens)\n\n# テキストファイルを読み込みトークン数を確認\ninput_file_path = './text.txt'\nwith open(input_file_path, 'r', encoding='utf-8') as file:\n    raw_text = file.read()\n\ntoken_len = count_tokens(raw_text)\nprint(token_len)\n\n```\n\n**結果例**\n\n- トークン数は1028\n\n---\n\n# 結論：目的に応じたツール選びを\n\nトークン数を確認する際のポイントをまとめると、以下のようになります。\n\n- **LLM用のトークナイザーが必要**なら`tiktoken`や`transformers`を使用\n- **汎用的な解析**なら`spaCy`や`MeCab`が便利\n\nトークン数がわかれば、トレーニングデータをどの程度追加する必要があるかや、モデルの特性に合った学習が可能になります。\n\n手軽にトークン数を確認し、効率的なLLM開発を目指しましょう。\n\n---\n","createdAt":"2025-05-19T08:50:58.242Z","updatedAt":"2025-06-06T00:00:57.810Z","publishedAt":"2025-06-06T00:00:57.823Z","docId":"h2wmt1yaodkt35gehandn5ym","tags":[{"id":20,"documentId":"fbllyhmskvu2l7wbg5dzpkri","createdAt":"2025-05-19T09:48:50.341Z","updatedAt":"2025-05-19T09:48:50.341Z","publishedAt":"2025-05-19T09:48:50.348Z","name":"LLM"},{"id":22,"documentId":"enpfq0z697dadtxijwpwwrsv","createdAt":"2025-05-19T09:49:17.862Z","updatedAt":"2025-05-19T09:49:17.862Z","publishedAt":"2025-05-19T09:49:17.870Z","name":"Python"}],"thumbnail":[{"id":24,"documentId":"yusr2e7tfjt4f1d3g3r3msjr","name":"20250513-20.png","alternativeText":null,"caption":null,"width":1504,"height":844,"formats":{"thumbnail":{"name":"thumbnail_20250513-20.png","hash":"thumbnail_20250513_20_67274fed95","ext":".png","mime":"image/png","path":null,"width":245,"height":137,"size":13.59,"sizeInBytes":13591,"url":"https://stg-raisex-tech-blog.s3.ap-northeast-1.amazonaws.com/thumbnail_20250513_20_67274fed95.png"},"medium":{"name":"medium_20250513-20.png","hash":"medium_20250513_20_67274fed95","ext":".png","mime":"image/png","path":null,"width":750,"height":421,"size":54.76,"sizeInBytes":54763,"url":"https://stg-raisex-tech-blog.s3.ap-northeast-1.amazonaws.com/medium_20250513_20_67274fed95.png"},"small":{"name":"small_20250513-20.png","hash":"small_20250513_20_67274fed95","ext":".png","mime":"image/png","path":null,"width":500,"height":281,"size":33.49,"sizeInBytes":33489,"url":"https://stg-raisex-tech-blog.s3.ap-northeast-1.amazonaws.com/small_20250513_20_67274fed95.png"},"large":{"name":"large_20250513-20.png","hash":"large_20250513_20_67274fed95","ext":".png","mime":"image/png","path":null,"width":1000,"height":561,"size":76.81,"sizeInBytes":76813,"url":"https://stg-raisex-tech-blog.s3.ap-northeast-1.amazonaws.com/large_20250513_20_67274fed95.png"}},"hash":"20250513_20_67274fed95","ext":".png","mime":"image/png","size":21.11,"url":"https://stg-raisex-tech-blog.s3.ap-northeast-1.amazonaws.com/20250513_20_67274fed95.png","previewUrl":null,"provider":"@strapi/provider-upload-aws-s3","provider_metadata":null,"createdAt":"2025-05-20T07:52:26.883Z","updatedAt":"2025-05-20T07:52:26.883Z","publishedAt":"2025-05-20T07:52:26.884Z"}]}},"__N_SSG":true}