{"pageProps":{"article":{"id":203,"documentId":"nyg7h5f0xw60bpk88glrh4zo","title":"LLMファインチューニングのチュートリアル","content":"近年、自然言語処理（NLP）の分野では大規模言語モデル（LLM）の活用が急速に進んでいます。特に、特定のタスク向けにモデルの性能を向上させる「ファインチューニング」は、プロジェクトでの需要が増えつつあります。\n\n本記事では、初心者の方でも理解しやすいように、ファインチューニングの基本的な流れを体験できるチュートリアルを用意しました。\n\n---\n\n# 環境設定と事前準備\n\nこのチュートリアルは、macOS環境（例: M2 MacBook Pro）をベースにしています。以下のコマンドで仮想環境を作成し、必要なライブラリをインストールしましょう。\n\n### 仮想環境のセットアップ\n\n```bash\npython -m venv .env\nsource .env/bin/activate\n```\n\n### 必要なライブラリのインストール\n\n以下のコマンドを実行して、必要なPythonライブラリをインストールします。\n\n```bash\npip install transformers datasets evaluate accelerate scikit-learn torch\n\n```\n\n### 動作確認\n\nライブラリが正しくインストールされたか確認するために、以下のテストコードを実行します。\n\n```bash\npython -c \"from transformers import pipeline; print(pipeline('sentiment-analysis')('we love you'))\"\n\n```\n\n期待される出力：\n\n```bash\n[{'label': 'POSITIVE', 'score': 0.9998704195022583}]\n\n```\n---\n\n# ファインチューニングスクリプトの作成と実行\n\nファインチューニングの手順を示したPythonスクリプト（`finetune.py`）を以下に示します。このスクリプトでは、[Yelpレビューのデータセット](https://huggingface.co/datasets/Yelp/yelp_review_full)を使用します。\n\n### `finetune.py`\n\n```python\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\nimport numpy as np\nimport evaluate\n\n# データセットの準備\ndataset = load_dataset(\"yelp_review_full\")\n\n# トークナイザーによるトークナイズ\ntokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\n\n# 学習データとテストデータ作成\nsmall_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(500))\nsmall_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(500))\n\n# モデルのロード\nmodel = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\", num_labels=5, torch_dtype=\"auto\")\n\n# 評価関数\nmetric = evaluate.load(\"accuracy\")\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    return metric.compute(predictions=predictions, references=labels)\n\n# 学習設定\ntraining_args = TrainingArguments(output_dir=\"test_trainer\", eval_strategy=\"epoch\")\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=small_train_dataset,\n    eval_dataset=small_eval_dataset,\n    compute_metrics=compute_metrics,\n)\n\n# 学習の実行\ntrainer.train()\n\n# モデルとトークナイザーの保存\nsave_dir = './finetuned/bert-base-cased'\ntokenizer.save_pretrained(save_dir)\nmodel.save_pretrained(save_dir)\n```\n\n### 実行結果\n\nスクリプトを実行すると、以下のような評価結果が表示されます。\n\n```bash\npython3 finetune.py\n\n{'eval_loss': 1.5970934629440308, 'eval_accuracy': 0.242, 'eval_runtime': 39.0603, 'eval_samples_per_second': 12.801, 'eval_steps_per_second': 1.613, 'epoch': 1.0}                                                                               \n{'eval_loss': 1.468526840209961, 'eval_accuracy': 0.346, 'eval_runtime': 36.1194, 'eval_samples_per_second': 13.843, 'eval_steps_per_second': 1.744, 'epoch': 2.0}                                                                                \n{'eval_loss': 1.4037985801696777, 'eval_accuracy': 0.368, 'eval_runtime': 35.7517, 'eval_samples_per_second': 13.985, 'eval_steps_per_second': 1.762, 'epoch': 3.0}                                                                               \n{'train_runtime': 467.6126, 'train_samples_per_second': 3.208, 'train_steps_per_second': 0.404, 'train_loss': 1.5324497121982474, 'epoch': 3.0}                                                                                                   \n100%|██████████████████████████████████████████████████████████████████████████████████| 189/189 [07:47<00:00,  2.47s/it]\n```\n\n---\n\n## ファインチューニングスクリプトの解説\n\n以下は、ファインチューニングスクリプト `finetune.py` の各パートを詳しく解説しまとめたものです。\n\n---\n\n### 1. データセットのロード\n\n```python\ndataset = load_dataset(\"yelp_review_full\")\n```\n\n### 解説：\n\n- Hugging Faceの`datasets`ライブラリを使用して、Yelpのレビューコメントデータセットをロードします。このデータセットにはレビューコメントと1~5の評価スコアが含まれています。\n\n### ポイント：\n\n- **データセットの種類**：Hugging Faceの`load_dataset`を使えば、豊富な事前構築データセットに簡単にアクセス可能。\n- **カスタムデータの利用**：自分のデータを使いたい場合、CSVやJSON形式で読み込むことも可能です。\n\n---\n\n### 2. トークナイズ\n\n```python\ntokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\n```\n\n### 解説：\n\n- トークナイザーを使ってテキストを数値データに変換します。これにより、モデルが扱える形式に整えられます。\n- `padding=\"max_length\"`で固定長の入力に揃え、`truncation=True`で最大トークン数を超えた部分をカットします。\n\n### ポイント：\n\n- **トークナイザーの選択**：モデルに対応するトークナイザーを使用することが重要（例: BERTモデルならBERT用のトークナイザー）。\n- **効率化**：`batched=True`を指定することで、複数のデータを一度にトークナイズして処理速度を向上。\n\n---\n\n### 3. データの分割と縮小\n\n```python\nsmall_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(500))\nsmall_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(500))\n\n```\n\n### 解説：\n\n- 訓練データと評価データを分割し、ファインチューニング用にサブセット（500件）を作成します。\n- `shuffle(seed=42)`はデータをランダムに並び替えますが、同じ結果を再現するためシード値を設定しています。\n\n### ポイント：\n\n- **データサイズの調整**：学習時間やリソースの制約に合わせてデータサイズを選択できます。\n- **小規模学習のメリット**：小規模データでもモデルの動作確認や理解が可能。\n\n---\n\n### 4. モデルのロード\n\n```python\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"google-bert/bert-base-cased\",\n    num_labels=5,\n    torch_dtype=\"auto\"\n)\n```\n\n### 解説：\n\n- Hugging Faceから事前学習済みのBERTモデルをロードし、分類タスク用に調整します。\n- `num_labels=5`は分類クラスの数（Yelpの評価スコア1~5）を指定しています。\n\n### ポイント：\n\n- **事前学習モデルの再利用**：大規模な事前学習済みモデルを活用することで、少ないデータでも良い結果が得られる。\n- **モデルのカスタマイズ**：分類クラス数や出力層を調整して特定のタスクに適応。\n\n---\n\n### 5. 評価関数の設定\n\n```python\nmetric = evaluate.load(\"accuracy\")\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    return metric.compute(predictions=predictions, references=labels)\n```\n\n### 解説：\n\n- `evaluate`ライブラリを使用して、モデルの精度（accuracy）を評価する関数を定義しています。\n- 推論結果（`logits`）を`argmax`で予測ラベルに変換し、実際のラベルと比較してスコアを計算します。\n\n### ポイント：\n\n- **評価指標の選択**：分類タスクでは`accuracy`が一般的ですが、タスクに応じて`precision`や`recall`も検討すべきです。\n- **簡単な実装**：`evaluate`ライブラリを使うと、一般的な評価指標を簡単に利用可能。\n\n---\n\n### 6. 学習設定\n\n```python\ntraining_args = TrainingArguments(\n    output_dir=\"test_trainer\",\n    eval_strategy=\"epoch\"\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=small_train_dataset,\n    eval_dataset=small_eval_dataset,\n    compute_metrics=compute_metrics,\n)\n\n```\n\n### 解説：\n\n- `TrainingArguments`で学習パラメータを指定します。ここでは、モデル保存先（`output_dir`）と評価タイミング（`eval_strategy=\"epoch\"`）を設定。\n- `Trainer`クラスは、学習プロセスを簡略化するための高レベルAPIです。\n\n### ポイント：\n\n- **簡単な学習管理**：`Trainer`を使うと、ループの作成や勾配計算の実装を気にする必要がありません。\n- **設定の柔軟性**：学習率やバッチサイズなどの詳細設定も可能です。\n\n---\n\n### 7. モデルの保存\n\n```python\ntokenizer.save_pretrained('./finetuned/bert-base-cased')\nmodel.save_pretrained('./finetuned/bert-base-cased')\n```\n\n### 解説：\n\n- ファインチューニング後のトークナイザーとモデルを保存します。この保存済みモデルは、将来的に推論やさらなる調整に使用できます。\n\n### ポイント：\n\n- **再利用性の向上**：モデルを保存しておくことで、他のプロジェクトやデプロイメントに活用可能。\n- **Hugging Face互換**：保存形式はHugging Faceライブラリでの再利用に最適化されています。\n\n---\n\n## チューニング前後のモデル性能比較\n\nファインチューニングによる効果を確認するため、同じ入力データを使ってモデルの予測結果を比較します。\n\n### チューニング前のモデルで推論\n\n以下のスクリプトで推論を行います。\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport numpy as np\n\nmodel_path = 'google-bert/bert-base-cased'\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_path)\n\ntext = \"I visit this bar at first time. Service is good. Making drinks and talking with Bartender is also good. I spend good 2 hours. I will come again next month.\"\ninputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\noutputs = model(**inputs)\npredicted_class = np.argmax(outputs.logits.detach().numpy(), axis=-1)\nprint(predicted_class)\n\n```\n\n出力結果：\n\n```bash\n[0] # 配列なので0始まり。0は評価1。\n```\n\ninputのtextからは4か5の高評価を期待しますが、評価が「1」となりました。\n\n### チューニング後のモデルで推論\n\n同じテキストを以下のスクリプトで推論します。\n\n```python\nmodel_path = './finetuned/bert-base-cased'\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_path)\n\ntext = \"I visit this bar at first time. Service is good. Making drinks and talking with Bartender is also good. I spend good 2 hours. I will come again next month.\"\ninputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\noutputs = model(**inputs)\npredicted_class = np.argmax(outputs.logits.detach().numpy(), axis=-1)\nprint(predicted_class)\n\n```\n\n出力結果：\n\n```bash\n[4]　# 配列なので0始まり。4は評価5。\n```\n\nこの結果は、より正確な評価となっており、モデルが改善されたことを示しています。\n\n## まとめ\n\nこのチュートリアルでは、`transformers`ライブラリを使用してLLMのファインチューニングを行い、わずか500件のデータでもモデルの性能が向上することを確認できました。\n\n本記事で紹介した手順は、あくまで学習目的です。\n\n本格的な案件では、より大規模なデータセットや高性能なハードウェアを利用することを推奨します。\n\n### 学びのポイント\n\n1. LLMの基本操作とトークナイズの仕組みが理解できる。\n2. Hugging Faceのエコシステムを活用する方法が習得できる。\n3. 簡単なファインチューニングによるモデルの改善効果を確認できる。\n\n今後のプロジェクトや個人学習にぜひ役立ててください！\n\n---\n\n# 参考\n\n- https://huggingface.co/docs/transformers/training\n- https://huggingface.co/docs/transformers/installation\n- https://huggingface.co/datasets/Yelp/yelp_review_full\n\n---\n\n","createdAt":"2025-05-19T08:50:57.925Z","updatedAt":"2025-06-02T00:00:52.772Z","publishedAt":"2025-06-02T00:00:52.785Z","docId":"y98x06tmbaz9hb7fleb7jpiq","tags":[{"id":20,"documentId":"fbllyhmskvu2l7wbg5dzpkri","createdAt":"2025-05-19T09:48:50.341Z","updatedAt":"2025-05-19T09:48:50.341Z","publishedAt":"2025-05-19T09:48:50.348Z","name":"LLM"},{"id":4,"documentId":"zc573y83cxbfql2umdwbhot9","createdAt":"2025-05-19T09:44:33.028Z","updatedAt":"2025-05-19T09:44:33.028Z","publishedAt":"2025-05-19T09:44:33.035Z","name":"Tutorial"},{"id":30,"documentId":"d99cu6c3t9hgwynppb92lhph","createdAt":"2025-05-19T09:50:59.080Z","updatedAt":"2025-05-19T09:50:59.080Z","publishedAt":"2025-05-19T09:50:59.086Z","name":"生成AI"}],"thumbnail":[{"id":19,"documentId":"qqgjmycd7ycoo6gwkv2zp97w","name":"20250513-15.png","alternativeText":null,"caption":null,"width":1504,"height":844,"formats":{"thumbnail":{"name":"thumbnail_20250513-15.png","hash":"thumbnail_20250513_15_a017e8a63e","ext":".png","mime":"image/png","path":null,"width":245,"height":137,"size":10.45,"sizeInBytes":10451,"url":"https://stg-raisex-tech-blog.s3.ap-northeast-1.amazonaws.com/thumbnail_20250513_15_a017e8a63e.png"},"large":{"name":"large_20250513-15.png","hash":"large_20250513_15_a017e8a63e","ext":".png","mime":"image/png","path":null,"width":1000,"height":561,"size":53.11,"sizeInBytes":53109,"url":"https://stg-raisex-tech-blog.s3.ap-northeast-1.amazonaws.com/large_20250513_15_a017e8a63e.png"},"medium":{"name":"medium_20250513-15.png","hash":"medium_20250513_15_a017e8a63e","ext":".png","mime":"image/png","path":null,"width":750,"height":421,"size":38.07,"sizeInBytes":38070,"url":"https://stg-raisex-tech-blog.s3.ap-northeast-1.amazonaws.com/medium_20250513_15_a017e8a63e.png"},"small":{"name":"small_20250513-15.png","hash":"small_20250513_15_a017e8a63e","ext":".png","mime":"image/png","path":null,"width":500,"height":281,"size":23.51,"sizeInBytes":23510,"url":"https://stg-raisex-tech-blog.s3.ap-northeast-1.amazonaws.com/small_20250513_15_a017e8a63e.png"}},"hash":"20250513_15_a017e8a63e","ext":".png","mime":"image/png","size":16.18,"url":"https://stg-raisex-tech-blog.s3.ap-northeast-1.amazonaws.com/20250513_15_a017e8a63e.png","previewUrl":null,"provider":"@strapi/provider-upload-aws-s3","provider_metadata":null,"createdAt":"2025-05-20T07:47:33.931Z","updatedAt":"2025-05-20T07:47:33.931Z","publishedAt":"2025-05-20T07:47:33.931Z"}]}},"__N_SSG":true}